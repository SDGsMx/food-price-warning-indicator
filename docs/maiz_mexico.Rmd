---
title: "Los precios del maíz en México"
output: hmtl_document
---

```{r opts, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "images/"
)
```


```{r, message=FALSE, warning=FALSE, comment=NA, results='hide'}
library(tidyverse)
library(lubridate)
library(stringr)
```

```{r, results='hide', message=FALSE}
tipo_cambio <-read_csv("data/tipo_de_cambio.csv") %>%
  mutate(fecha = dmy(fecha)) %>%
  mutate(año = year(fecha))  %>%
  mutate(mes = month(fecha)) %>%
  select(año,mes,tipo_cambio)
```


```{r, results='hide', message=FALSE}
internacional <- read_csv("data/precio_internacional_dolares.csv") %>%
  separate(Month, c("mes", "año"), " ")  %>%
  mutate(año = as.numeric(año))  %>%
  filter(año>2000) %>%
  mutate(mes = match(mes,month.abb))  %>%
  mutate(fecha = make_datetime(year=año, month=mes, day=1)) %>%
  mutate(fecha= ymd(fecha)) %>%
  rename(int_price = Price) %>%
  left_join(tipo_cambio) %>%
  # Obtenemos el precio por kilogramo (De tonelada) por el tipo de cambio
  mutate(int_price = (int_price*tipo_cambio)/1000) %>%
  select(fecha,año,mes,int_price)
```


# Precios nacionales promedio por estado:

```{r, results='hide', message=FALSE, comment=NA, warning=FALSE}
nacional <- read_csv("data/precios_granos_semanales.csv")
nacional <- select(nacional,producto,precio_min,fecha,edo_destino,obs) %>%
  mutate(fecha=dmy(fecha)) %>% # tipo de fecha
  mutate(precio_min = ifelse(precio_min > 8, NA, precio_min)) # identificamos un outlier
```


```{r, message=FALSE, warning=FALSE}
estados_dic <- read_csv("data/estados_dic.csv") %>%
  rename(edo_destino=NOM_ENT) %>%
  mutate(edo_destino = str_to_lower(edo_destino)) %>%
  mutate(CVE_ENT = str_pad(CVE_ENT, 2, pad = "0"))
```


```{r, message=FALSE, warning=FALSE}
maiz_nacional <- read_csv("data/precios_granos_semanales.csv") %>%
  select(producto,fecha,edo_destino,precio_min,obs) %>%
  #definimos variable de fecha
  mutate(fecha=dmy(fecha)) %>%
  #filtramos el outlier
  filter(precio_min < 15) %>%
  arrange(fecha) %>%
  mutate(mes = month(fecha)) %>%
  mutate(año = year(fecha)) %>%
  mutate(fecha = make_datetime(year=año,month=mes,1)) %>%
  mutate(fecha = ymd (fecha)) %>%
  left_join(estados_dic,by = "edo_destino") %>%
    mutate(CVE_ENT = ifelse(edo_destino== "michoacán", "16",
                          ifelse(edo_destino== "veracruz","30",
                                 ifelse(edo_destino=="df","09",
                                        ifelse(edo_destino=="coahuila","05",CVE_ENT)))))
```


```{r}
nacional <- maiz_nacional %>%
  group_by(fecha,año,mes) %>%
  summarise(precio_promedio = mean(precio_min, na.rm = TRUE)) %>%
  left_join(internacional, by = c("fecha", "año", "mes"))
nacional_2 <- nacional %>%
  ungroup() %>% as_data_frame() %>%
  select(fecha, precio_promedio, int_price)
colnames(nacional_2) <- c("Fecha","Promedio nacional", "Precio internacional")
nacional_2 <- nacional_2 %>%
  gather(key = Precios, value = Precio, -Fecha) %>%
  filter(!is.na(Precio))
ggplot(nacional_2) +
  geom_line(aes(x = Fecha, y = Precio, color = Precios)) +
  scale_x_date(date_breaks = "6 months", date_labels = "%b %y") +
  theme(legend.position="bottom", axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
# Obtener una base tipo panel para cada estado del precio por mes del maiz blanco
semantic2 <- maiz_nacional %>%
  ungroup() %>% as.data.frame() %>%
  select(CVE_ENT,fecha,precio_min) %>%
  group_by(fecha,CVE_ENT) %>%
  summarise(precio_promedio = mean(precio_min,na.rm=TRUE)) %>%
  spread(key = CVE_ENT, value = precio_promedio) %>%
  left_join(nacional%>%ungroup()%>%select(fecha,int_price), estatal, by = c("fecha")) %>%
  arrange(fecha) %>%
  filter(!is.na(int_price)) %>%
  filter(fecha >= ymd("2005-05-01")) %>%
  select(fecha,`01`,`03`:`11`,`13`:`26`,`28`:int_price) %>%
  ungroup %>% as.data.frame() %>%
  gather(key = CVE_ENT, value = precio_promedio, -fecha, -int_price) %>%
  mutate(CVE_ENT = as.integer(CVE_ENT)) %>%
  rename(cve_ent = CVE_ENT, precio_internacional=int_price, precio_ent = precio_promedio)
```

Contamos con los precios del maíz para el tiempo $i$ y el estado $j$ que denotamos por $p_{ij}$. También contamos con los precios internacionales en el tiempo $i$, que denotamos por $f_i$.

El modelo propuesto es:
$$
p_{ij} \sim N(\mu_{ij}, \tau_p)
$$
donde
$$
\mu_{ij} = g(f_i, i, j) = h_1(f_i) + h_2(i) + h_3(j),
$$
donde $h_1,h_2,h_3$ son funciones lineales y proponemos que
$$
h_1(f_i) = \alpha_0 + \alpha_1 f_{i-1} + \alpha_2 f_{i-2} + \alpha_3 f_{i-3}
$$
$$
h_2(i) = \gamma_i
$$
$$
h_3(j) = \beta_j
$$


```{r, message=FALSE, warning=FALSE}
library(R2jags)
library(gridExtra)
```

# Datos

```{r}
semantic_ancha = semantic2 %>%
  spread(key = cve_ent, value = precio_ent)
head(semantic_ancha)
```

```{r}
f <- semantic_ancha %>% select(precio_internacional)
f <- as.numeric(f$precio_internacional)
head(f)
```

```{r}
length(f)
```

```{r}
precios <- semantic_ancha %>%
  select(-precio_internacional) %>%
  arrange(fecha)
head(precios)
```

```{r}
P <- as.matrix(select(precios, -fecha))
head(P)
```

```{r}
dim(P)
```

# Modelo

```{r}
modelo_ar.txt <-
'
model
{
  #Likelihood
  # Space eq
  for(i in 4:(n-r)){
    for(j in 1:m){
      P[i,j] ~ dnorm(mu[i,j], tau.p)
      mu[i,j] <-  alpha1*f[i-1] + alpha2*f[i-2] + alpha3*f[i-3] + gamma[i] + beta[j]
    }
    gamma[i] ~ dnorm(gamma[i-1], tau.g)
    alpha3[i] ~ dnorm(alpha3[i-1], tau.g)
  }
  #priors
  alpha0 ~ dnorm(0,0.001)
  alpha1 ~ dnorm(0,0.001)
  alpha2 ~ dnorm(0,0.001)
  alpha3 ~ dnorm(0,0.001)
  for(i in 1:3){
    gamma[i] ~ dnorm(0, 0.001)
  }
  for(j in 1:m){
    beta[j] ~ dnorm(0,0.001)
  }
  tau.p ~ dgamma(0.001,0.001)
  tau.g ~ dgamma(0.001,0.001)
  #prediccion 1
  for(i in 1:(n-3-r)){
    for(j in 1:m){
      yf[i,j] ~ dnorm(mu[i+3,j], tau.p)
    }
  }
  #prediccion 2
  for(i in 1:r){
    for(j in 1:m){
      yp[i,j] ~ dnorm(mup[i,j], tau.p)
      mup[i,j] <- alpha0 + alpha1*f[n+i-1] + alpha2*f[n+i-2] + alpha3*f[n+i-3] + gamma[n-r+i] + beta[j]
    }
    gamma[n-r+i] ~ dnorm(gamma[n-r+i-1], tau.g)
  }
}
'
cat(modelo_ar.txt, file = '../scripts/models/modelo_ar.bugs')
```

Parámetros iniciales:

```{r}
n <- length(f)
m <- dim(P)[2]
r <- 1
data <- list("n" = n,
             "m" = m,
             "r" = r,
             "P" = P,
             "f" = f)
inits <- function(){list(alpha0 = 0,
                         alpha1 = 0,
                         alpha2 = 0,
                         alpha3 = 0,
                         tau.p = 0.01,
                         tau.g = 0.4,
                         gamma = rep(0.2,n),
                         beta = rep(0,m),
                         yf = matrix(rep(0, (n-3-r)*m), ncol = m),
                         yp = matrix(rep(0, r*m), ncol = m)
                    )}
parameters <- c('alpha0',
                'alpha1',
                'alpha2',
                'alpha3',
                'beta',
                'gamma',
                'tau.p',
                'yf',
                'yp')
```


Ajuste del modelo:

```{r,warning=FALSE,message=FALSE}
jags_fit <- jags(
  model.file = "../scripts/models/modelo_ar.bugs",    # modelo de JAGS
  inits = inits,   # valores iniciales
  data = data,    # lista con los datos
  parameters.to.save = parameters,  # parámetros por guardar
  n.chains = 5,   # número de cadenas
  n.iter = 10000,  # número de pasos
  n.burnin = 1000, # calentamiento de la cadena
  n.thin = 1,
  jags.seed = 1234
)
```


Ahora vamos a evaluar los resultados. Lo primero que tenemos que hacer es revisar la convergencia de la cadena. Para esto vemos la siguiente gráfica para el parámetro $\alpha_1$:

```{r,echo=FALSE}
analisis_estimacion <- function(z,title){
  g1 <- ggplot(data.frame(est=1:length(z),z=z),aes(x=est,y=z))+
    geom_line(color='hotpink')
  g2 <- ggplot(data.frame(est=1:length(z),z=z),aes(x=est,
              y=cumsum(z)/(1:length(z))))+
    geom_line(color='hotpink') + ylab('')
  g3 <- ggplot(data.frame(z),aes(x=z))+
    geom_histogram(aes(y=..density..),colour = 'darkviolet', fill = 'lightpink',bins=30) +
    geom_density() +
    geom_vline(xintercept = c(quantile(z,0.01),quantile(z,0.99),mean(z)),
               size = 1.2,color = c('hotpink4','hotpink4','deeppink')) +
    scale_x_continuous(limits = c(quantile(z,0.02),quantile(z,0.98)))
  lag <- (1:round(+10*log(length(z),10)))-1
  bacf <- acf(z, plot = FALSE)
  bacfdf <- data.frame(lag=bacf$lag,acf=bacf$acf)
  ciline <- qnorm((1 - 0.95)/2)/sqrt(length(z))
  g4 <- ggplot(data = bacfdf, mapping = aes(x = lag, y = acf)) +
        geom_hline(aes(yintercept = 0)) +
        geom_segment(mapping = aes(xend = lag, yend = 0),color='navyblue') +
          geom_hline(yintercept = -ciline, color = "mediumorchid",size = 0.2) +
          geom_hline(yintercept = ciline, color = "mediumorchid", size = 0.2) +
          theme(title=element_text(size=10))
  grid.arrange(g1,g2,g3,g4,ncol=2,nrow=2,top=title)
}
```


```{r,echo=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
out <- jags_fit$BUGSoutput$sims.list
z <- out$alpha1
analisis_estimacion(z,'alpha1')
```


# Predicciones

```{r}
#Predicciones
out.sum <- jags_fit$BUGSoutput$summary
out.yp <- out.sum[grep("yp",rownames(out.sum)),]
```


# Cálculo de errores por entidad

```{r}
entidades <- colnames(P)
observados <- P[n,]
pred <- out.yp[,'mean']
errores <- data.frame(entidad=as.numeric(entidades), observado=observados, pred=pred)
estados_dic <- estados_dic %>% mutate(entidad=parse_number(CVE_ENT))
errores <- errores %>% left_join(estados_dic, by = "entidad") %>%
  mutate(error = (pred - observado)^2)
```

```{r, message=FALSE, warning=FALSE}
library(rgeos)
library(maptools)
library(rgdal)
library(ggmap)
```

```{r, message=FALSE, warning=FALSE}
ent_shp <- readOGR("data/shp" , "entidad")
ent_df <- fortify(ent_shp, region = "CVE_ENT")
ent_err <- ent_df %>%
  mutate(CVE_ENT = id) %>%
  left_join(errores, by = "CVE_ENT")
ggplot() +
  geom_polygon(data = ent_err, aes(long, lat, group = group, fill = error)) +
  labs(title = "Error cuadrático por entidad", fill = "Error") +
  scale_fill_gradient(low = "white", high = "red") +
  coord_fixed()
```



```{r}
media_entidad <- function(ent){
  regex <- paste0('yf\\[([0-9]*)\\,',as.character(ent),'\\]')
  out.yf <- out.sum[str_detect(pattern=regex, string=rownames(out.sum)),]
  return(tibble(media = mean(out.yf[,1]), inf = mean(out.yf[,3]), sup = mean(out.yf[,7])))
}
medias_ent <- purrr::map_df(.x = 1:24, .f = media_entidad)
medias_ent$entidad <- as.numeric(colnames(P))
ent_error_cuant <- ent_err %>% left_join(medias_ent, by = "entidad")
```


```{r}
media_mes <- function(mes){
  regex <- paste0('yf\\[',as.character(mes),'\\,([0-9]*)\\]')
  out.yf <- out.sum[str_detect(pattern=regex, string=rownames(out.sum)),]
  return(tibble(media = mean(out.yf[,1],na.rm=T),
                inf = mean(out.yf[,3],na.rm=T),
                sup = mean(out.yf[,7],na.rm=T)))
}
medias_mes <- purrr::map_df(.x = 1:134, .f = media_mes)
fechas <- precios$fecha[4:137]
medias_mes$fecha <- fechas
```


# Entidad 1

```{r, message=FALSE, warning=FALSE}
semantic_1 <- semantic2 %>% filter(cve_ent == 1)
semantic_1 <- semantic_1 %>% left_join(medias_mes, by = 'fecha')
ggplot(semantic_1) +
  geom_point(aes(x=fecha, y = precio_ent), color = "grey80", size = 2) +
  geom_line(aes(x=fecha, y=media), color = "navyblue", linetype = 1) +
  geom_line(aes(x=fecha, y=inf), color = "navyblue", linetype = 2) +
  geom_line(aes(x=fecha, y=sup), color = "navyblue", linetype = 2) +
  geom_point(data = data.frame(fecha=ymd('2016-10-01'),media=mean(pred)),
            aes(x=fecha, y=media), color = 'red') +
  geom_segment(x = as.numeric(ymd('2016-10-01')), y = mean(out.yp[,3]),
               xend = as.numeric(ymd('2016-10-01')), yend = mean(out.yp[,7]),
               color = 'red', alpha = 0.8) +
  scale_x_date(date_breaks = "6 months", date_labels = "%b %y") +
  theme(legend.position="bottom", axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("Precio del maíz") + ggtitle("Ajuste y predicción del precio del maíz")
```

# Discusión de los resultados

El ajuste es bueno pero las predicciones futuras tienen demasiada varianza. En realidad no es clara la interpretación del parámetro $\gamma_i$ o cómo se estima cuando los datos ya no son observados. Si necesito tres observaciones hacia atrás del precio internacional es lógico que las estimaciones sean cada vez menos precisas, pero no contamos con un modelo para estimar el precio internacional.


# Consideraciones

Pensamos que sería buena idea modelar las diferencias para cada entidad
$$
d_{ij} = p_{ij} - p_{i-1,j}.
$$
Consideramos la posibilidad de utilizar las diferencias entre el precio internacional y el precio actual en cada estado para cada tiempo $f_i - p_{ij}$ como una opción que permita modelar mejor el precio en cada entidad, pero por ahora no lo incluimos en el modelo.

Otros aspectos que podrían ser útiles son

* incluir información climática, como precipitación temporal

* datos sobre el subsidio a la producción del maíz de Sagarpa

Otros objetivos que tenemos en mente son:

* estimar el costo del coyote que podría calcularse como la diferencia entre el precio a nivel de parcela y el precio de la central de abastos más cercana para cada municipio. Se puede además comparar esta diferencia con los kilómetros de distancia entre la parcela y la central de abastos y estudiar si existe una posible asociación entre la distancia y la diferencia en el precio.

Proponemos ahora modelar el precio desde el 2005 hasta el 2015 y ajustar la serie y predecir para todos los meses del 2016, y luego analizar los errores para cada mes por cada entidad. Otro aspecto relevante es qué tanta historia hacia atrás es necesaria para lograr una buena predicción del precio actual y el precio futuro.

El siguiente paso es modelar el precio internacional a partir de otras variables. Otra opción sería utilizar un modelo no temporal utilizando información observada de otras fuentes y el precio internacional pasado y el precio observado pasado de cada entidad.
