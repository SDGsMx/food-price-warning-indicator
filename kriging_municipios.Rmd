---
title: "Los precios del maíz en México"
output: hmtl_document
---



```{r opts, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "images/"
)
```

 
```{r, message=FALSE, warning=FALSE, comment=NA, results='hide'}
library(tidyverse)
library(lubridate)
library(stringr)
```

```{r, results='hide', message=FALSE}
tipo_cambio <-read_csv("data/tipo_de_cambio.csv") %>%
  mutate(fecha = dmy(fecha)) %>%
  mutate(año = year(fecha))  %>%
  mutate(mes = month(fecha)) %>%
  select(año,mes,tipo_cambio)
```


```{r, results='hide', message=FALSE}
internacional <- read_csv("data/precio_internacional_dolares.csv") %>% 
  separate(Month, c("mes", "año"), " ")  %>%
  mutate(año = as.numeric(año))  %>%
  filter(año>2000) %>%
  mutate(mes = match(mes,month.abb))  %>%
  mutate(fecha = make_datetime(year=año, month=mes, day=1)) %>%
  mutate(fecha= ymd(fecha)) %>%
  rename(int_price = Price) %>% 
  left_join(tipo_cambio) %>%
  # Obtenemos el precio por kilogramo (De tonelada) por el tipo de cambio
  mutate(int_price = (int_price*tipo_cambio)/1000) %>% 
  select(fecha,año,mes,int_price) 
```


# Precios nacionales promedio por estado:

```{r, results='hide', message=FALSE, comment=NA, warning=FALSE}
nacional <- read_csv("data/precios_granos_semanales.csv")
nacional <- select(nacional,producto,precio_min,fecha,edo_destino,obs) %>% 
  mutate(fecha=dmy(fecha)) %>% # tipo de fecha
  mutate(precio_min = ifelse(precio_min > 8, NA, precio_min)) # identificamos un outlier
```


```{r, message=FALSE, warning=FALSE}
estados_dic <- read_csv("data/estados_dic.csv") %>%
  rename(edo_destino=NOM_ENT) %>% 
  mutate(edo_destino = str_to_lower(edo_destino)) %>%
  mutate(CVE_ENT = str_pad(CVE_ENT, 2, pad = "0"))
```


```{r, message=FALSE, warning=FALSE}
maiz_nacional <- read_csv("data/precios_granos_semanales.csv") %>%
  select(producto,fecha,edo_destino,precio_min,obs) %>% 
  #definimos variable de fecha
  mutate(fecha=dmy(fecha)) %>%
  #filtramos el outlier
  filter(precio_min < 15) %>%  
  arrange(fecha) %>%
  mutate(mes = month(fecha)) %>% 
  mutate(año = year(fecha)) %>% 
  mutate(fecha = make_datetime(year=año,month=mes,1)) %>% 
  mutate(fecha = ymd (fecha)) %>%
  left_join(estados_dic,by = "edo_destino") %>%
    mutate(CVE_ENT = ifelse(edo_destino== "michoacán", "16", 
                          ifelse(edo_destino== "veracruz","30",
                                 ifelse(edo_destino=="df","09",
                                        ifelse(edo_destino=="coahuila","05",CVE_ENT))))) 
```


```{r}
nacional <- maiz_nacional %>%
  group_by(fecha,año,mes) %>% 
  summarise(precio_promedio = mean(precio_min, na.rm = TRUE)) %>% 
  left_join(internacional,by = c("fecha", "año", "mes")) 
nacional_2 <- nacional %>%
  ungroup() %>% as_data_frame() %>%
  select(fecha, precio_promedio, int_price) 
colnames(nacional_2) <- c("Fecha","Promedio nacional", "Precio internacional")
nacional_2 <- nacional_2 %>% 
  gather(key = Precios, value = Precio, -Fecha) %>%
  filter(!is.na(Precio))
ggplot(nacional_2) + 
  geom_line(aes(x = Fecha, y = Precio, color = Precios)) + 
  scale_x_date(date_breaks = "6 months", date_labels = "%b %y") + 
  theme(legend.position="bottom", axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
# Obtener una base tipo panel para cada estado del precio por mes del maiz blanco
semantic2 <- maiz_nacional %>%
  ungroup() %>% as.data.frame() %>%
  select(CVE_ENT,fecha,precio_min) %>%
  group_by(fecha,CVE_ENT) %>%
  summarise(precio_promedio = mean(precio_min,na.rm=TRUE)) %>%
  spread(key = CVE_ENT, value = precio_promedio) %>%
  left_join(nacional%>%ungroup()%>%select(fecha,int_price), estatal, by = c("fecha")) %>%
  arrange(fecha) %>%
  filter(!is.na(int_price)) %>%
  filter(fecha >= ymd("2005-05-01")) %>%
  select(fecha,`01`,`03`:`11`,`13`:`26`,`28`:int_price) %>%
  ungroup %>% as.data.frame() %>%
  gather(key = CVE_ENT, value = precio_promedio, -fecha, -int_price) %>%
  mutate(CVE_ENT = as.integer(CVE_ENT)) %>%
  rename(cve_ent = CVE_ENT, precio_internacional=int_price, precio_ent = precio_promedio)
```

<!-- 
```{r, message=FALSE, warning=FALSE}
library(R2jags)
library(gridExtra)
```


El objetivo planteado es hacer un modelo dinámico de segundo orden:

Sea $i$ el tiempo en el que se observa la medición del precio del maíz y $j$ el estado correspondiente a la observación. Para $i=1,2,\ldots,n$ y $j=1,2,\ldots,m$ se tiene que

$$
\begin{aligned}
\mu_{ij} &= \alpha_{i(j)} + \beta_j + \gamma_i f_i, \quad p_{ij} \sim N(\mu_{ij},\tau_{p}), \\
\eta_{i(j)} &= \alpha_{i-1(j)}, \quad \alpha_{i(j)} \sim N(\eta_{i(j)},\tau_\alpha).
\end{aligned}
$$

Las distribuciones iniciales son:
$$
\begin{aligned}
\tau_p &= 1/\sigma_p^2, \\
\sigma_p &\sim \mbox{Gamma}(1,0.01), \\
\beta_j,\gamma_i &\sim N(0,0.001), \\
\tau_\alpha &= 1/\sigma_\alpha^2,\\
\sigma_\alpha &\sim \mbox{Gamma}(1,0.01)
\end{aligned}
$$


Parámetros iniciales:
```{r}
fn <- data.frame(fn=1:length(unique(semantic2$fecha)), fecha=unique(semantic2$fecha))
semantic <- semantic2 %>% left_join(fn, by = "fecha")
m <- length(unique(semantic$CVE_ENT))
entidad_recode <- data.frame(CVE_ENT=unique(semantic$CVE_ENT),rec = 1:m)
semantic <- semantic %>% left_join(entidad_recode, by = "CVE_ENT")
r <- 6
semantic_obs <- semantic %>% filter(fn <= length(unique(semantic2$fecha)) - r)
n <- length(unique(semantic_obs$fecha))
j1 <- semantic_obs$rec
p <- semantic_obs$precio_promedio
f1 <- unique(semantic_obs$int_price)
fn1 <- semantic_obs$fn
semantic_pred <- semantic %>% filter(fn > n)
semantic_pred_2 <- semantic_pred %>%
  select(fn,int_price) %>%
  distinct(fn,int_price) %>%
  arrange(fn)
grd <- expand.grid(rec = unique(semantic_pred$rec), fn = unique(semantic_pred$fn)) %>%
  left_join(semantic_pred_2, by = "fn")
f2 <- unique(grd$int_price)
j2 <- grd$rec
fn2 <- grd$fn - n
obs <- length(p)
pred <- m*r
data <- list("n" = n, "m" = m, "r" = r,
             "j1" = j1,
             "j2" = j2,
             "p" = p,
             "f1" = f1,
             "f2" = f2,
             "obs" = obs, "pred" = pred,
             "fn1" = fn1,
             "fn2" = fn2)
inits <- function(){list(alpha=rep(0,n),
                         alpha2=rep(0,r),
                         tau.p=0.01,
                         tau.a=0.01,
                         yf1=rep(0,obs),
                         beta=rep(0,m),
                         gamma1=rep(0,n),
                         gamma2=rep(0,r),
                         yf2=rep(0,pred)
                    )}
parameters <- c('alpha','alpha2','beta','gamma1','gamma2','tau.p','tau.a','yf1','yf2')
```

```{r}
modelo.txt <-
'
model
{
  #Likelihood
  # Space eq
  for(i in 1:obs){
    p[i] ~ dnorm(mu[i],tau.p)
    mu[i] <- alpha[fn1[i]] + beta[j1[i]] + gamma1[fn1[i]] * f1[fn1[i]]
  }
  #state eq
  for(i in 2:n){
    alpha[i] ~ dnorm(mu.a[i],tau.a)
    mu.a[i] <- mu.a[j1[i-1]]
  }
  #priors
  alpha[1] ~ dnorm(mu.a[1],0.001)
  mu.a[1] <- 0
  tau.p ~ dgamma(0.001,0.001)
  tau.a ~ dgamma(0.001,0.001)
  for(i in 1:n){
    gamma1[i] ~ dnorm(0,0.001)
  }
  for(k in 1:m){
    beta[k] ~ dnorm(0,0.001)
  }
  # prediccion 1
  for(i in 1:obs){
    yf1[i] ~ dnorm(mu[i],tau.p)
  }
  #prediccion2
  for(i in 1:pred){
    yf2[i] ~ dnorm(mu2[i],tau.p)
    mu2[i] <- alpha2[fn2[i]] + beta[j2[i]] + gamma2[fn2[i]] * f2[fn2[i]]
  }
  for(i in 2:r){
    alpha2[i] ~ dnorm(mu.a2[i],tau.a)
    mu.a2[i] <-  mu.a2[i-1]
  }
  alpha2[1] ~ dnorm(mu.a2[1],tau.a)
  mu.a2[1] <- alpha[n]
  for(i in 1:r){
    gamma2[i] ~ dnorm(0,0.001)
  }
}
'
cat(modelo.txt, file = 'modelo.bugs')
```



```{r,warning=FALSE,message=FALSE}
jags_fit <- jags(
  model.file = "modelo.bugs",    # modelo de JAGS
  inits = inits,   # valores iniciales
  data = data,    # lista con los datos
  parameters.to.save = parameters,  # parámetros por guardar
  n.chains = 5,   # número de cadenas
  n.iter = 2500,    # número de pasos
  n.burnin = 250,   # calentamiento de la cadena
  n.thin = 7
)
```


Ahora vamos a evaluar los resultados. Lo primero que tenemos que hacer es revisar la convergencia de la cadena. Para esto vemos la siguiente gráfica para la devianza:

```{r,echo=FALSE,cache=TRUE}
analisis_estimacion <- function(z,title){
  g1 <- ggplot(data.frame(est=1:length(z),z=z),aes(x=est,y=z))+
    geom_line(color='hotpink') 
  g2 <- ggplot(data.frame(est=1:length(z),z=z),aes(x=est,
              y=cumsum(z)/(1:length(z))))+
    geom_line(color='hotpink') + ylab('')
  g3 <- ggplot(data.frame(z),aes(x=z))+
    geom_histogram(aes(y=..density..),colour = 'darkviolet', fill = 'lightpink',bins=30) +
    geom_density() +
    geom_vline(xintercept = c(quantile(z,0.01),quantile(z,0.99),mean(z)),
               size = 1.2,color = c('hotpink4','hotpink4','deeppink')) +
    scale_x_continuous(limits = c(quantile(z,0.02),quantile(z,0.98)))
  lag <- (1:round(+10*log(length(z),10)))-1
  bacf <- acf(z, plot = FALSE)
  bacfdf <- data.frame(lag=bacf$lag,acf=bacf$acf)
  ciline <- qnorm((1 - 0.95)/2)/sqrt(length(z))
  g4 <- ggplot(data = bacfdf, mapping = aes(x = lag, y = acf)) +
        geom_hline(aes(yintercept = 0)) +
        geom_segment(mapping = aes(xend = lag, yend = 0),color='navyblue') +
          geom_hline(yintercept = -ciline, color = "mediumorchid",size = 0.2) +
          geom_hline(yintercept = ciline, color = "mediumorchid", size = 0.2) +
          theme(title=element_text(size=10))
  grid.arrange(g1,g2,g3,g4,ncol=2,nrow=2,top=title)
}
```


```{r,echo=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
out <- jags_fit$BUGSoutput$sims.list
z <- out$deviance
analisis_estimacion(z,'deviance')
```


Se puede ver que la devianza es aproximadamente 3450.

El DIC es 
```{r}
jags_fit$BUGSoutput$DIC
```

Analicemos los precios ajustados para la entidad 1:

```{r,echo=FALSE,fig.width=5,fig.height=3, echo=FALSE, fig.height=3, fig.width=5, message=FALSE, warning=FALSE}
out.sum <- jags_fit$BUGSoutput$summary

#Predictions
out.yf<-out.sum[grep("yf1",rownames(out.sum)),]
media <- out.yf[,1]; inf <- out.yf[,3]; sup <- out.yf[,7];
semantic_2 <- semantic_obs %>% 
  select(fn,rec,precio_promedio,int_price) %>%
  bind_cols(data.frame(media,inf,sup)) %>%
  filter(rec == 1) %>%
  select(fn,rec,precio_promedio,int_price,media,inf,sup)
out.yf2 <- out.sum[grep("yf2",rownames(out.sum)),]
media2 <- out.yf2[,1]; inf2 <- out.yf2[,3]; sup2 <- out.yf2[,7];
semantic_3 <- grd %>%
  left_join(semantic_pred%>%select(fn,rec,precio_promedio), by = c("fn","rec")) %>%
  bind_cols(data.frame(media=media2,inf=inf2,sup=sup2)) %>%
  filter(rec == 1) %>%
  select(fn,rec,precio_promedio,int_price,media,inf,sup)
ggplot(semantic_2) +
  geom_point(aes(x=fn, y = precio_promedio), color = "grey80", size = 2) + 
  geom_line(aes(x=fn, y=media), color = "navyblue", linetype = 1) +
  geom_line(aes(x=fn, y=inf), color = "navyblue", linetype = 2) +
  geom_line(aes(x=fn, y=sup), color = "navyblue", linetype = 2) +
  geom_point(data=semantic_3, aes(x = fn, y = precio_promedio), color = "red") +
  geom_line(data=semantic_3, aes(x=fn, y=media), color = "darkslateblue", linetype = 1) #+
  #geom_line(data=semantic_3, aes(x=fn, y=inf), color = "darkslateblue", linetype = 2) +
  #geom_line(data=semantic_3, aes(x=fn, y=sup), color = "darkslateblue", linetype = 2)
```


# Análisis espacial

Esto lo queremos hacer con el fin únicamente de interpolar utilizando un método estadístico simple


 -->

Contamos con los precios del maíz para el tiempo $i$ y el estado $j$ que denotamos por $p_{ij}$. También contamos con los precios internacionales en el tiempo $i$, que denotamos por $f_i$.

El modelo propuesto es:
$$
p_{ij} \sim N(\mu_{ij}, \tau_p)
$$
donde 
$$
\mu_{ij} = g(f_i, i, j) = h_1(f_i) + h_2(i) + h_3(j), 
$$
donde $h_1,h_2,h_3$ son funciones lineales y proponemos que
$$
h_1(f_i) = \alpha_0 + \alpha_1 f_{i-1} + \alpha_2 f_{i-2} + \alpha_3 f_{i-3}
$$
$$
h_2(i) = \gamma_i
$$
$$
h_3(j) = \beta_j
$$


```{r, message=FALSE, warning=FALSE}
library(R2jags)
library(gridExtra)
```

# Datos

```{r}
semantic_ancha = semantic2 %>%
  spread(key = cve_ent, value = precio_ent)
head(semantic_ancha)
```

```{r}
f <- semantic_ancha %>% select(precio_internacional)
f <- as.numeric(f$precio_internacional)
head(f)
```

```{r}
length(f)
```

```{r}
precios <- semantic_ancha %>% 
  select(-precio_internacional) %>%
  arrange(fecha)
head(precios)
```

```{r}
P <- as.matrix(select(diferencias, -fecha))
head(P)
```

```{r}
dim(P)
```

# Modelo

```{r}
modelo_ar.txt <-
'
model
{
  #Likelihood
  # Space eq
  for(i in 4:(n-r-5)){
    for(j in 1:m){
      P[i,j] ~ dnorm(mu[i,j], tau.p)
      mu[i,j] <- alpha0 + alpha1*f[i-1] + alpha2*f[i-2] + alpha3*f[i-3] + gamma[i] + beta[j]
    }
  }
  #priors
  alpha0 ~ dnorm(0,0.001)
  alpha1 ~ dnorm(0,0.001)
  alpha2 ~ dnorm(0,0.001)
  alpha3 ~ dnorm(0,0.001)
  for(i in 1:n){
    gamma[i] ~ dnorm(0, 0.001)
  }
  for(j in 1:m){
    beta[j] ~ dnorm(0,0.001)
  }
  tau.p ~ dgamma(0.001,0.001)
  #prediccion 1
  for(i in 1:(n-3-r-5)){
    for(j in 1:m){
      yf[i,j] ~ dnorm(mu[i+3,j], tau.p)
    }
  }
  #prediccion 2
  for(i in 1:r){
    for(j in 1:m){
      yp[i,j] ~ dnorm(mup[i,j], tau.p)
      mup[i,j] <- alpha0 + alpha1*f[n-5+i-1] + alpha2*f[n-5+i-2] + alpha3*f[n-5+i-3] + gamma[n-r-5+i] + beta[j]
    }
  }
}
'
cat(modelo_ar.txt, file = 'modelo_ar.bugs')
```

Parámetros iniciales:

```{r}
n <- length(f)
m <- dim(P)[2]
r <- 6
data <- list("n" = n, "m" = m, "r" = r,
             "P" = P,
             "f" = f)
inits <- function(){list(alpha0 = 0,
                         alpha1 = 0,
                         alpha2 = 0,
                         alpha3 = 0,
                         tau.p = 0.01,
                         gamma = rep(0,n),
                         beta = rep(0,m),
                         yf = matrix(rep(0, (n-3-r-5)*m), ncol = m),
                         yp = matrix(rep(0, r*m), ncol = m)
                    )}
parameters <- c('alpha0','alpha1','alpha2','alpha3','beta','gamma','tau.p','yf','yp')
```


Ajuste del modelo:

```{r,warning=FALSE,message=FALSE}
jags_fit <- jags(
  model.file = "modelo_ar.bugs",    # modelo de JAGS
  inits = inits,   # valores iniciales
  data = data,    # lista con los datos
  parameters.to.save = parameters,  # parámetros por guardar
  n.chains = 1,   # número de cadenas
  n.iter = 10000,  # número de pasos
  n.burnin = 1000, # calentamiento de la cadena
  n.thin = 1,
  jags.seed = 1234
)
```


Ahora vamos a evaluar los resultados. Lo primero que tenemos que hacer es revisar la convergencia de la cadena. Para esto vemos la siguiente gráfica para la devianza:

```{r,echo=FALSE,cache=TRUE}
analisis_estimacion <- function(z,title){
  g1 <- ggplot(data.frame(est=1:length(z),z=z),aes(x=est,y=z))+
    geom_line(color='hotpink') 
  g2 <- ggplot(data.frame(est=1:length(z),z=z),aes(x=est,
              y=cumsum(z)/(1:length(z))))+
    geom_line(color='hotpink') + ylab('')
  g3 <- ggplot(data.frame(z),aes(x=z))+
    geom_histogram(aes(y=..density..),colour = 'darkviolet', fill = 'lightpink',bins=30) +
    geom_density() +
    geom_vline(xintercept = c(quantile(z,0.01),quantile(z,0.99),mean(z)),
               size = 1.2,color = c('hotpink4','hotpink4','deeppink')) +
    scale_x_continuous(limits = c(quantile(z,0.02),quantile(z,0.98)))
  lag <- (1:round(+10*log(length(z),10)))-1
  bacf <- acf(z, plot = FALSE)
  bacfdf <- data.frame(lag=bacf$lag,acf=bacf$acf)
  ciline <- qnorm((1 - 0.95)/2)/sqrt(length(z))
  g4 <- ggplot(data = bacfdf, mapping = aes(x = lag, y = acf)) +
        geom_hline(aes(yintercept = 0)) +
        geom_segment(mapping = aes(xend = lag, yend = 0),color='navyblue') +
          geom_hline(yintercept = -ciline, color = "mediumorchid",size = 0.2) +
          geom_hline(yintercept = ciline, color = "mediumorchid", size = 0.2) +
          theme(title=element_text(size=10))
  grid.arrange(g1,g2,g3,g4,ncol=2,nrow=2,top=title)
}
```


```{r,echo=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
out <- jags_fit$BUGSoutput$sims.list
z <- out$deviance
analisis_estimacion(z,'deviance')
```


# Predicciones

```{r}
#Predicciones
out.sum <- jags_fit$BUGSoutput$summary
out.yf <- out.sum[grep("yf",rownames(out.sum)),]
out.yp <- out.sum[grep("yp",rownames(out.sum)),]
```


# Entidad 1

```{r, message=FALSE, warning=FALSE}
yf_1 <- as.data.frame(out.yf[1:(n-3-r-5),])
yp_1 <- as.data.frame(out.yp[1:r,])
semantic_1 <- semantic2 %>% filter(cve_ent == 1)
yf <- bind_rows(yf_1, yp_1)
yf$fecha <- semantic_1$fecha[4:(n-5)]
semantic_1 <- semantic_1 %>% left_join(yf, by = 'fecha')
ggplot(subset(semantic_1, fecha <= ymd('2015-11-01'))) +
  geom_point(aes(x=fecha, y = precio_ent), color = "grey80", size = 2) + 
  geom_line(aes(x=fecha, y=mean), color = "navyblue", linetype = 1) +
  geom_line(aes(x=fecha, y=`2.5%`), color = "navyblue", linetype = 2) +
  geom_line(aes(x=fecha, y=`97.5%`), color = "navyblue", linetype = 2) + 
  geom_line(data = subset(semantic_1, fecha>ymd('2015-11-01')), 
            aes(x=fecha, y=mean), color = 'red') +
  geom_point(data = subset(semantic_1, fecha>ymd('2015-11-01')), 
            aes(x=fecha, y=precio_ent), color = 'black') +
  scale_x_date(date_breaks = "6 months", date_labels = "%b %y") + 
  theme(legend.position="bottom", axis.text.x = element_text(angle = 45, hjust = 1))
```

# Discusión de los resultados

El ajuste es bueno pero las predicciones futuras tienen demasiada varianza. En realidad no es clara la interpretación del parámetro $\gamma_i$ o cómo se estima cuando los datos ya no son observados. Si necesito tres observaciones hacia atrás del precio internacional es lógico que las estimaciones sean cada vez menos precisas, pero no contamos con un modelo para estimar el precio internacional.


# Consideraciones

Pensamos que sería buena idea modelar las diferencias para cada entidad 
$$
d_{ij} = p_{ij} - p_{i-1,j}.
$$
Consideramos la posibilidad de utilizar las diferencias entre el precio internacional y el precio actual en cada estado para cada tiempo $f_i - p_{ij}$ como una opción que permita modelar mejor el precio en cada entidad, pero por ahora no lo incluimos en el modelo.

Otros aspectos que podrían ser útiles son

* incluir información climática, como precipitación temporal

* datos sobre el subsidio a la producción del maíz de Sagarpa

Otros objetivos que tenemos en mente son:

* estimar el costo del coyote que podría calcularse como la diferencia entre el precio a nivel de parcela y el precio de la central de abastos más cercana para cada municipio. Se puede además comparar esta diferencia con los kilómetros de distancia entre la parcela y la central de abastos y estudiar si existe una posible asociación entre la distancia y la difrencia en el precio.

Algunas ideas sobre fuentes externas que pueden ser útiles son las siguientes:

* el paquete de R "quantmod" 

* datos de "index mundi"



